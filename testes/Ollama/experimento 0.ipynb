{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descrição do experimento:\n",
    " \n",
    "Esse experimento terá a seguinte estrutura:\n",
    "\n",
    "Modelo LLM: LLama3.1\n",
    "\n",
    "Modelo Embedding: nomic-embed-text\t\n",
    "\n",
    "Texto chunkenizado: não\n",
    "\n",
    "EIOS Resumido: não\n",
    "\n",
    "Notícia Resumida: não\n",
    "\n",
    "Método de escolha: sem escolha (passando todo o contexto)\n",
    "\n",
    "Tamanho de entrada do Llama3.1: 128k\n",
    "\n",
    "Tamanho médio do prompt usando todos os textos do EIOS Test: +900k\n",
    "\n",
    "Média de palavras por linha no dataset de teste: 506.07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1890\n"
     ]
    }
   ],
   "source": [
    "#Lendo datasets e configurando numpy para float64 \n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.float_ = np.float64\n",
    "\n",
    "data = pd.read_csv('EIOS_train.csv')\n",
    "train = pd.DataFrame(data)\n",
    "data = pd.read_csv('EIOS_test.csv')\n",
    "test = pd.DataFrame(data)\n",
    "print(len(test))\n",
    "# print(train.label.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506.07\n"
     ]
    }
   ],
   "source": [
    "# pegando a média de palavras por linha\n",
    "\n",
    "total = len(test)\n",
    "\n",
    "soma = ''\n",
    "\n",
    "for text in test.text:\n",
    "    soma += text\n",
    "\n",
    "print(f\"{len(soma)/total:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.2.2) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "# Definindo o modelo de LLM\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model = \"llama3.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo o modelo gerador de embeddings\n",
    "\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando os documentos, e gerando os embeddings a partir dos documentos\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "\n",
    "# Carregando os documentos de treino\n",
    "loader = DataFrameLoader(test, page_content_column=\"text\") \n",
    "docs = loader.load()\n",
    "\n",
    "# Gerando os embeddings\n",
    "# vectorstore = Chroma.from_documents(documents=docs, embedding=embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import ast \n",
    "\n",
    "# Define the function to call the Ollama Llama3.1 model\n",
    "def ollama_llm(question, context):\n",
    "    prompt_specs = \"\"\"\n",
    "        Classifique se as questões são relevantes ou irrelevantes para o contexto da área de saúde.\n",
    "        Pense da perspectiva de um profissional de saúde como um médico, mas também como um epidemiologista, enfermeiro, ou outro profissional de saúde.\n",
    "\n",
    "        Você deve classificar cada questão em uma das 2 categorias:\n",
    "        1. Relevante\n",
    "        2. Irrelevante\n",
    "\n",
    "        Para cada questão, forneça uma justificativa detalhada para a escolha feita.\n",
    "        Certifique-se de que a justificativa seja clara e esteja relacionada ao contexto da área da saúde.\n",
    "\n",
    "        Faça:\n",
    "        1. Seja específico e claro.\n",
    "        2. Compreenda o contexto da questão relacionada à saúde antes de categorizá-la.\n",
    "        3. Aja como um profissional de saúde.\n",
    "\n",
    "        NÃO faça:\n",
    "        1. Não adivinhe ou invente informações.\n",
    "        2. Não crie novas categorias de justificativa; utilize apenas as fornecidas acima.\n",
    "\n",
    "        Dica: Classifique como irrelevante quaisquer questões que não estejam diretamente relacionadas à saúde ou cuidados médicos.\n",
    "\n",
    "        O formato de saída deve ser o seguinte:\n",
    "        {\n",
    "            \"classe\": \"\",\n",
    "            \"justificativa\": \"\"\n",
    "        }\n",
    "\n",
    "        A classe deve ser uma string das categorias acima: 'Relevante' ou 'Irrelevante'.\n",
    "        A justificativa deve ser um texto explicando o motivo da escolha da categoria.\n",
    "\n",
    "        Exemplos:\n",
    "        1. Questão: 'Tenho dores pulmonares afetadas pelo cigarro.'\n",
    "            Saída: { 'classe': 'Relevante', 'justificativa': 'O texto trata de um problema respiratório diretamente relacionado à área da saúde.' }\n",
    "\n",
    "        2. Questão: 'Como posso aumentar minha produtividade no trabalho?'\n",
    "            Saída: { 'classe': 'Irrelevante', 'justificativa': 'A questão não está relacionada diretamente ao contexto da área da saúde.' }\n",
    "\n",
    "        3. Questão: 'Tenho dores nas costas após longas horas sentado.'\n",
    "            Saída: { 'classe': 'Relevante', 'justificativa': 'A questão aborda um problema de saúde ocupacional que afeta a coluna vertebral.' }\n",
    "\n",
    "        4. Questão: 'Quais são os melhores livros de autoajuda?'\n",
    "            Saída: { 'classe': 'Irrelevante', 'justificativa': 'A questão não se relaciona diretamente com o contexto de saúde ou cuidados médicos.' }\n",
    "        \n",
    "        Não se esqueça das orientações e forneça a resposta no formato de saída indicado.\n",
    "\n",
    "        A saída deve ser estritamente no formato indicado, com aspas simples e chaves.\n",
    "\n",
    "        A classificação deve ser exclusivamente entre 'Relevante' e 'Irrelevante'.\n",
    "    \"\"\"\n",
    "    \n",
    "    formatted_prompt = f\"{prompt_specs}\\nQuestão: {question}\\n\\nContexto: {context}\"\n",
    "    \n",
    "    # with open('formatted_prompt.txt', 'w') as file:       \n",
    "    #     file.write(formatted_prompt)\n",
    "    # file.close()\n",
    "\n",
    "    response = ollama.chat(model='llama3.1', messages=[{'role': 'user', 'content': formatted_prompt}])\n",
    "    return response['message']['content']\n",
    "\n",
    "def rag_chain(question):\n",
    "    all_docs_content = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    return ollama_llm(question, all_docs_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def redimensionar_df(df, n):\n",
    "    data_test = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        data_test.append(row['text'])\n",
    "        if len(data_test) == n:\n",
    "            break\n",
    "\n",
    "    df_data_test = pd.DataFrame(data_test, columns=['text'])\n",
    "    return df_data_test\n",
    "\n",
    "df_test = redimensionar_df(train, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(df):\n",
    "    correct = 0\n",
    "    for text in tqdm(df['text']):\n",
    "        # print(text)\n",
    "        response = rag_chain(text)\n",
    "        response = ast.literal_eval(response)\n",
    "        choice = response['classe'].lower()\n",
    "        if choice == 'relevante':\n",
    "            correct += 1\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:35<?, ?it/s]\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<unknown>, line 1)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3577\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[16], line 1\u001b[0m\n    correct_test = run_test(df_test)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[8], line 6\u001b[0m in \u001b[1;35mrun_test\u001b[0m\n    response = ast.literal_eval(response)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m/usr/lib/python3.10/ast.py:64\u001b[0m in \u001b[1;35mliteral_eval\u001b[0m\n    node_or_string = parse(node_or_string.lstrip(\" \\t\"), mode='eval')\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m/usr/lib/python3.10/ast.py:50\u001b[0;36m in \u001b[0;35mparse\u001b[0;36m\n\u001b[0;31m    return compile(source, filename, mode, flags,\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m<unknown>:1\u001b[0;36m\u001b[0m\n\u001b[0;31m    O texto não apresenta apenas uma tarefa ou questão específica para resolver, mas sim um conjunto de informações e notícias relacionadas a diferentes temas. No entanto, posso ajudar com alguns aspectos destes textos. Aqui estão algumas coisas que você pode estar procurando:\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "correct_test = run_test(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Correct: {correct_test}/{len(df_test)}\")\n",
    "print(f\"Accuracy: {correct_test/len(df_test)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
